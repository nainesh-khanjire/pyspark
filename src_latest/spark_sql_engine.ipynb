{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07a5089-d538-42dd-bd02-a988c0728d94",
   "metadata": {},
   "source": [
    "## SPARK SQL ENGINE\n",
    "\n",
    "> structured API execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689b424-9e03-44bf-adfc-de183e9236a2",
   "metadata": {},
   "source": [
    "## QUESTIONS  >>\n",
    "\n",
    "1.  what is catalyst optomizer and spark sql analyzer?\n",
    "2.  why do we get analysis exception error ?\n",
    "3.  what is catalog ?\n",
    "4.  what is physical planning / spark plan ?\n",
    "5.  is spark sql engine a compiler -- yes it converts code into java byte code\n",
    "6.  how many phases are involved in spark sql engine to convert code into java byte code ? -- 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963239e2-6072-4818-aada-5fae3585253d",
   "metadata": {},
   "source": [
    "# SPARK SQL ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b726498-5c1d-449c-ab89-84987002d541",
   "metadata": {},
   "source": [
    "we can write 3 code >\n",
    "1. SQL\n",
    "2. Dataframe API\n",
    "3. Dataset\n",
    "\n",
    "at high level >\n",
    "\n",
    "all of above code goes to catalyst optimizer (SPARK SQL ENGINE) which converts code into java byte code (RDD)\n",
    "SPARK SQL ENGINE has 4 phases :\n",
    "1. Analysis\n",
    "2. logical planning\n",
    "3. physical planning\n",
    "4. code conversion\n",
    "\n",
    "> flow of execution in spark >\n",
    "\n",
    "CODE --> UNRESOLVED LOGICAL PLAN -->(analysis)--> RESOLVED LOGICAL PLAN --> (logical optimization) --> OPTIMIZED LOGICAL PLAN --> PHYSICAL PLAN --> (cost model) --> BEST PHYSICAL PLAN --> FINAL CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac11442-d1d8-4423-b783-646e86a65ffb",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfb0cb-e1fe-4648-8a18-67fde83f6598",
   "metadata": {},
   "source": [
    "### what is RDD?\n",
    "1. resilient distributed dataset\n",
    "2. it is a data structure\n",
    "3. spark loads cluster store data into RAM\n",
    "4.  the data is distributed across the clusters\n",
    "> resilient : in case of failure it recreates the data, fault tolerance\n",
    "> distributed : over the cluster\n",
    ">  dataset: actual data\n",
    "> RDD is immutable\n",
    "> DAG is created  for actions and transformations\n",
    "> lineage knows hwo to create a RDD again in case of failure\n",
    "> lazy, immutable, optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8b47f-e2c3-4dd1-8fe3-c7ba16b01dc0",
   "metadata": {},
   "source": [
    "### When do we need RDD ?\n",
    "\n",
    "> Disadvantage:\n",
    "1. no optimization done by , we need to control\n",
    "2. how to, what to we need to configure all of this by ourselves\n",
    "\n",
    "> Advantage:\n",
    "1. best for unstructured data\n",
    "2. type safe for data, compile time safety, error before runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44397aac-f5d8-4a53-a943-2f7af15fb8d1",
   "metadata": {},
   "source": [
    "### Why we should not use RDD ?\n",
    "> too much code for simple transformations\n",
    "> RDD : How to\n",
    "> Dataframe : What to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9597f8-544f-4764-b9bd-8277309ad1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46202b-31b9-4492-8535-cedd32aa09b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
